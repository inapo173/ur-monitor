import requests
from bs4 import BeautifulSoup
import os
import time
import re
import datetime
import random
import sys

# ==========================================
# 1. ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šã‚¨ãƒªã‚¢
# ==========================================

# ç›£è¦–ã—ãŸã„ç‰©ä»¶ã®URLãƒªã‚¹ãƒˆ
# â˜…æ™®é€šã®URL(.html)ã‚’å…¥ã‚Œã¦ãŠã‘ã°ã€è‡ªå‹•ã§è£ãƒ«ãƒ¼ãƒˆã‚’æ¢ã—ã«è¡Œãã¾ã™
TARGET_URLS = [
    # ç¦ä½ä¸€ä¸ç›®
    "https://www.ur-net.go.jp/chintai/kanto/tokyo/20_2660.html",
    # æœ¨å ´å…¬åœ’ä¸‰å¥½ä½å®…
    "https://www.ur-net.go.jp/chintai/kanto/tokyo/20_3450.html",
    # æœ¨å ´å…¬åœ’å¹³é‡ä½å®…
    "https://www.ur-net.go.jp/chintai/kanto/tokyo/20_3570.html",
    # æœ¨å ´ä¸‰ä¸ç›®ãƒ‘ãƒ¼ã‚¯ãƒã‚¤ãƒ„
    "https://www.ur-net.go.jp/chintai/kanto/tokyo/20_3860.html",
    # å¤§å³¶å…­ä¸ç›®
    "https://www.ur-net.go.jp/chintai/kanto/tokyo/20_1920.html",
    # é«˜å³¶å¹³å›£åœ°
    "https://www.ur-net.go.jp/chintai/kanto/tokyo/20_2250.html",
    # ã‚³ãƒ³ãƒ•ã‚©ãƒ¼ãƒ«å’Œå…‰è¥¿å¤§å’Œ
    "https://www.ur-net.go.jp/chintai/kanto/saitama/50_4120.html",
    # å¿—æ‘ä¸€ä¸ç›®
    "https://www.ur-net.go.jp/chintai/kanto/tokyo/20_1190.html",
    # å¤§äº•å…­ä¸ç›®
    "https://www.ur-net.go.jp/chintai/kanto/tokyo/20_1830.html",
    # å—åƒä½ä¸ƒä¸ç›®ãƒã‚¤ãƒ„
    "https://www.ur-net.go.jp/chintai/kanto/tokyo/20_4290.html",
    # ä¸Šé¦¬äºŒä¸ç›®
    "https://www.ur-net.go.jp/chintai/kanto/tokyo/20_2400.html",
    # å°å³¶ç”ºäºŒä¸ç›®
    "https://www.ur-net.go.jp/chintai/kanto/tokyo/20_2540.html",
    # æ±å››ãƒ„æœ¨äºŒä¸ç›®
    "https://www.ur-net.go.jp/chintai/kanto/tokyo/20_2640.html",
    # å¤§è°·ç”°ä¸€ä¸ç›®
    "https://www.ur-net.go.jp/chintai/kanto/tokyo/20_2810.html",
    # åŒ—ç ‚äº”ä¸ç›®
    "https://www.ur-net.go.jp/chintai/kanto/tokyo/20_2820.html",
    # åŒ—ç ‚ä¸ƒä¸ç›®
    "https://www.ur-net.go.jp/chintai/kanto/tokyo/20_2940.html",
    # ç¥ç”°å°å·ç”ºãƒã‚¤ãƒ„
    "https://www.ur-net.go.jp/chintai/kanto/tokyo/20_3820.html",
    # æ–°è“®æ ¹
    "https://www.ur-net.go.jp/chintai/kanto/tokyo/20_4760.html",
    # ã‚¢ã‚¯ã‚·ã‚¹æ±å››ãƒ„æœ¨
    "https://www.ur-net.go.jp/chintai/kanto/tokyo/20_5840.html",
    # è‘›è¥¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¿ã‚¦ãƒ³æ¸…æ–°ãƒ—ãƒ©ã‚¶
    "https://www.ur-net.go.jp/chintai/kanto/tokyo/20_3480.html",
    # å·å£èŠåœ’å›£åœ°
    "https://www.ur-net.go.jp/chintai/kanto/saitama/50_1820.html",
    # ã‚¢ãƒ¼ãƒãƒ³ãƒ©ã‚¤ãƒ•è¥¿æ–°äº•
    "https://www.ur-net.go.jp/chintai/kanto/tokyo/20_5320.html",
    
    # === ãƒ†ã‚¹ãƒˆç”¨ç‰©ä»¶ ===
    "https://www.ur-net.go.jp/chintai/kanto/saitama/50_1270.html"
]

# â˜…â˜…â˜… ãƒ†ã‚¹ãƒˆä¸­ï¼ˆ30ä¸‡ï¼‰ â˜…â˜…â˜… é€šçŸ¥ãŒæ¥ãŸã‚‰ 85000 ã«æˆ»ã—ã¦ãã ã•ã„
MAX_RENT_LIMIT = 300000

# GitHub Secretsã‹ã‚‰èª­ã¿è¾¼ã‚€
DISCORD_WEBHOOK_URL = os.environ.get("DISCORD_WEBHOOK_URL")
HEALTHCHECK_URL = os.environ.get("HEALTHCHECK_URL", "")

# ==========================================
# 2. ã‚·ã‚¹ãƒ†ãƒ é–¢æ•°ç¾¤
# ==========================================

def send_discord(message):
    if not DISCORD_WEBHOOK_URL:
        print("âš  ã€é‡è¦ã€‘Discord URLãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return
    try:
        requests.post(DISCORD_WEBHOOK_URL, json={"content": message})
        print(">> Discordé€šçŸ¥é€ä¿¡å®Œäº†")
    except Exception as e:
        print(f"é€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")

def extract_room_details(soup):
    """
    HTMLã®ä¸­ã‹ã‚‰éƒ¨å±‹æƒ…å ±ã‚’æ¢ã—å‡ºã™é–¢æ•°
    """
    rooms = []
    candidates = soup.find_all(['tr', 'div', 'li', 'dd'])
    seen_identifiers = set()

    for element in candidates:
        text = element.get_text()
        clean_text = re.sub(r'\s+', ' ', text).strip()
        
        # 1. å®¶è³ƒã®ã€Œå¹…ï¼ˆï½ï¼‰ã€ãŒã‚ã‚‹è¡Œã¯ç„¡è¦–ï¼ˆç›®å®‰å®¶è³ƒã‚’é™¤å¤–ï¼‰
        if "ã€œ" in clean_text or "ï½" in clean_text or "range" in clean_text:
            continue

        # 2. ã€Œå·å®¤ã€ãŒãªã„è¡Œã¯ç„¡è¦–
        if "å·å®¤" not in clean_text:
            continue

        # 3. ã€Œå††ã€ãŒãªã„è¡Œã¯ç„¡è¦–
        if "å††" not in clean_text:
            continue

        # ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        rent_match = re.search(r'([0-9,]+)\s?å††', clean_text)
        room_num_match = re.search(r'([0-9\-]+å·æ£Ÿ[0-9]+å·å®¤|[0-9]+å·å®¤)', clean_text)
        
        if rent_match and room_num_match:
            room_number = room_num_match.group(1)
            
            if room_number in seen_identifiers:
                continue
            
            rent_str = rent_match.group(1).replace(",", "")
            try:
                rent = int(rent_str)
            except:
                continue
            
            # å®¶è³ƒãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            if rent > MAX_RENT_LIMIT:
                continue
            
            size_match = re.search(r'([0-9]+)\s?(ã¡|m2)', clean_text)
            floor_match = re.search(r'([0-9]+)\s?éš', clean_text)
            type_match = re.search(r'[0-9]?[LDKSR]+', clean_text)

            room_info = {
                "number": room_number,
                "rent_fmt": rent_match.group(0),
                "size": size_match.group(0) if size_match else "ä¸æ˜",
                "floor": floor_match.group(0) if floor_match else "ä¸æ˜",
                "type": type_match.group(0) if type_match else "-"
            }
            rooms.append(room_info)
            seen_identifiers.add(room_number)

    return rooms

def get_ajax_url(soup, original_url):
    """
    HTMLå†…ã®ç§˜å¯†ã®æš—å·ï¼ˆinitSearchï¼‰ã‚’è¦‹ã¤ã‘ã¦ã€è£APIã®URLã‚’ä½œã‚‹
    """
    scripts = soup.find_all("script")
    for script in scripts:
        if script.string and "initSearch" in script.string:
            # initSearch('50', '127', '0') ã®ã‚ˆã†ãªæ•°å­—ã‚’æ¢ã™
            match = re.search(r"initSearch\('(\d+)',\s*'(\d+)',\s*'(\d+)'\)", script.string)
            if match:
                shisya = match.group(1) # ä¾‹: 50
                danchi = match.group(2) # ä¾‹: 127
                shubetsu = match.group(3) # ä¾‹: 0
                
                # URã®è£APIï¼ˆãƒ‡ãƒ¼ã‚¿ã®ã‚ã‚Šã‹ï¼‰ã®URLã‚’ä½œæˆ
                # ãƒ‘ã‚¿ãƒ¼ãƒ³: /chintai/api/bukken/detail/dtl_50_127_0.html
                api_url = f"https://www.ur-net.go.jp/chintai/api/bukken/detail/dtl_{shisya}_{danchi}_{shubetsu}.html"
                print(f"   (è£APIã‚’ç™ºè¦‹: {api_url})")
                return api_url
    return None

def check_vacancy(url):
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    ]
    headers = {
        "User-Agent": random.choice(user_agents),
        "Referer": "https://www.ur-net.go.jp/"
    }
    
    try:
        # 1. ã¾ãšæ™®é€šã®URLã«ã‚¢ã‚¯ã‚»ã‚¹
        response = requests.get(url, headers=headers, timeout=30)
        if response.encoding is None or response.encoding == 'ISO-8859-1':
            response.encoding = response.apparent_encoding

        if response.status_code != 200:
            print(f"âš  ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•— ({response.status_code}): {url}")
            return False

        soup = BeautifulSoup(response.content, "html.parser")
        page_text = soup.get_text()

        # 2. æº€å®¤ãƒã‚§ãƒƒã‚¯ï¼ˆãŠè¾å„€ç”»é¢ï¼‰
        if "æ²è¼‰ã¯çµ‚äº†ã„ãŸã—ã¾ã—ãŸ" in page_text or "ãŠæ¢ã—ã®ãƒšãƒ¼ã‚¸ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“" in page_text:
            print(f"â†’ æº€å®¤ (æ²è¼‰çµ‚äº†ç”»é¢): {url}")
            return False

        # 3. ã¾ãšã¯ã“ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰éƒ¨å±‹ã‚’æ¢ã™
        rooms = extract_room_details(soup)
        
        # 4. ã‚‚ã—éƒ¨å±‹ãŒè¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ã€è£APIï¼ˆéš ã—ãƒšãƒ¼ã‚¸ï¼‰ã‚’æ¢ã—ã«è¡Œã
        if not rooms:
            # "initSearch" ã¨ã„ã†æš—å·ã‚’æ¢ã—ã¦ã€è£URLã‚’ä½œã‚‹
            api_url = get_ajax_url(soup, url)
            
            if api_url:
                # è£APIã«ã‚¢ã‚¯ã‚»ã‚¹
                try:
                    time.sleep(1) # å„ªã—ãã‚¢ã‚¯ã‚»ã‚¹
                    api_response = requests.get(api_url, headers=headers, timeout=30)
                    api_response.encoding = 'utf-8' # APIã¯ã ã„ãŸã„UTF-8
                    
                    if api_response.status_code == 200:
                        api_soup = BeautifulSoup(api_response.content, "html.parser")
                        # è£ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚‚ã†ä¸€åº¦éƒ¨å±‹ã‚’æ¢ã™
                        rooms = extract_room_details(api_soup)
                except Exception as e:
                    print(f"   (è£APIã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {e})")

        # 5. çµæœåˆ¤å®š
        if not rooms:
            print(f"â†’ æ¡ä»¶ã«åˆã†ç©ºãéƒ¨å±‹ãªã—ï¼ˆã¾ãŸã¯äºˆç®—ã‚ªãƒ¼ãƒãƒ¼ï¼‰: {url}")
            return False

        title = soup.find("h1")
        area_name = title.get_text(strip=True) if title else "ä¸æ˜ãªå›£åœ°"
        
        msg = f"**ã€URç©ºå®¤ç™ºè¦‹ï¼ã€‘**\nTarget: {area_name}\nURL: {url}\n\n"
        for i, room in enumerate(rooms):
            if i >= 5:
                msg += "ã»ã‹è¤‡æ•°ä»¶ã‚ã‚Š...\n"
                break
            msg += f"ãƒ»{room['number']} | {room['type']} | {room['floor']} | **{room['rent_fmt']}**\n"
        
        send_discord(msg)
        return True

    except Exception as e:
        print(f"ä¾‹å¤–ç™ºç”Ÿ ({url}): {e}")
        return False

# ==========================================
# 3. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯
# ==========================================
if __name__ == "__main__":
    print("--- ç›£è¦–ã‚¸ãƒ§ãƒ–é–‹å§‹ ---")
    
    if DISCORD_WEBHOOK_URL:
        print("âœ… Discordè¨­å®š: OK")
    else:
        print("âŒ Discordè¨­å®š: æœªè¨­å®š")

    wait_time = random.randint(5, 15)
    print(f"Wait for {wait_time} sec...")
    time.sleep(wait_time)
    
    found_any_in_this_run = False
    
    for url in TARGET_URLS:
        if not url: continue
        is_found = check_vacancy(url)
        if is_found:
            found_any_in_this_run = True
        time.sleep(2)

    now_utc = datetime.datetime.now(datetime.timezone.utc)
    if now_utc.hour == 14 and now_utc.minute >= 25:
        if not found_any_in_this_run:
            summary_msg = "ğŸ **ã€æœ¬æ—¥ã®ç›£è¦–çµ‚äº†ã€‘**\n23:30ã®å®šæ™‚é€£çµ¡ã§ã™ã€‚\næœ¬æ—¥ã¯æ¡ä»¶ã«åˆã†ç©ºãç‰©ä»¶ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\nã¾ãŸæ˜æ—¥8:00ã‹ã‚‰ç›£è¦–ã‚’å†é–‹ã—ã¾ã™ã€‚"
            send_discord(summary_msg)

    if HEALTHCHECK_URL:
        try:
            requests.get(HEALTHCHECK_URL, timeout=10)
            print("Healthchecks Pingé€ä¿¡å®Œäº†")
        except:
            pass
            
    print("--- ç›£è¦–ã‚¸ãƒ§ãƒ–çµ‚äº† ---")
